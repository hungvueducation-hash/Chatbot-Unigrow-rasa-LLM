# Troubleshooting Guide - Unigrow AI Chatbot

H∆∞·ªõng d·∫´n x·ª≠ l√Ω c√°c l·ªói th∆∞·ªùng g·∫∑p khi setup, develop, v√† run Unigrow Chatbot.

---

## üî¥ Setup Issues

### 1. Error: "Python not found"

**L·ªói:**
```
'python' is not recognized as an internal or external command
```

**Nguy√™n nh√¢n:** Python ch∆∞a ƒë∆∞·ª£c add v√†o PATH

**Fix:**
```bash
# Windows - Add Python to PATH
setx PATH "%PATH%;C:\Python310"

# Restart terminal

# Verify
python --version
```

---

### 2. Error: "venv activation failed"

**L·ªói:**
```
cannot be loaded because running scripts is disabled on this system
```

**Nguy√™n nh√¢n:** PowerShell execution policy

**Fix:**
```powershell
# Windows PowerShell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# R·ªìi activate l·∫°i
venv\Scripts\activate
```

---

### 3. Error: "Permission denied" when activating venv

**L·ªói:**
```
command not found: venv/bin/activate
```

**Nguy√™n nh√¢n:** Linux/Mac permissions

**Fix:**
```bash
# C·∫•p quy·ªÅn execute
chmod +x venv/bin/activate

# Activate
source venv/bin/activate
```

---

## üü° Installation Issues

### 4. Error: "pip install -r requirements.txt" fails

**L·ªói:**
```
ERROR: Could not find a version that satisfies the requirement rasa==3.6.21
```

**Nguy√™n nh√¢n:** Network issue ho·∫∑c package kh√¥ng t∆∞∆°ng th√≠ch

**Fix:**
```bash
# Upgrade pip tr∆∞·ªõc
python -m pip install --upgrade pip

# C√†i l·∫°i
pip install -r requirements.txt -v

# N·∫øu v·∫´n l·ªói, c√†i t·ª´ng package:
pip install rasa==3.6.21
pip install torch==2.8.0
pip install mistral-7b-instruct==latest
```

---

### 5. Error: "torch installation fails"

**L·ªói:**
```
ERROR: No matching distribution found for torch==2.1.0
```

**Nguy√™n nh√¢n:** CPU/GPU mismatch

**Fix:**
```bash
# For GPU (CUDA 11.8)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# For CPU only
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

---

## üî¥ Ollama Issues

### 6. Error: "Ollama connection refused"

**L·ªói:**
```
Error: Failed to connect to http://localhost:11434
```

**Nguy√™n nh√¢n:** Ollama service kh√¥ng ch·∫°y

**Fix:**
```bash
# Kh·ªüi ƒë·ªông Ollama
ollama serve

# Ki·ªÉm tra trong terminal m·ªõi
ollama list
```

---

### 7. Error: "Cannot pull mistral model"

**L·ªói:**
```
Error: Head "https://registry.ollama.ai/...": dial tcp: lookup registry.ollama.ai: no such host
```

**Nguy√™n nh√¢n:** Network issue ho·∫∑c registry down

**Fix:**
```bash
# C√°ch 1: Th·ª≠ l·∫°i
ollama pull mistral

# C√°ch 2: Reset Ollama
ollama serve
# (Ctrl+C)
ollama serve  # Start again

# C√°ch 3: Ki·ªÉm tra DNS
nslookup registry.ollama.ai

# C√°ch 4: D√πng VPN n·∫øu c·∫ßn
# B·∫≠t VPN, r·ªìi:
ollama pull mistral
```

---

### 8. Error: "Mistral model loading fails"

**L·ªói:**
```
Error loading model: out of memory
```

**Nguy√™n nh√¢n:** GPU memory kh√¥ng ƒë·ªß

**Fix:**
```bash
# Ki·ªÉm tra GPU memory
nvidia-smi

# N·∫øu < 4GB, d√πng model nh·∫π h∆°n:
ollama pull tinyllama

# Ho·∫∑c close c√°c ·ª©ng d·ª•ng kh√°c
```

---

## üü° Rasa Issues

### 9. Error: "rasa train fails"

**L·ªói:**
```
YamlValidationException: Failed to validate 'domain.yml'
Cannot find required key 'mappings'. Path: '/slots/user_age'
```

**Nguy√™n nh√¢n:** domain.yml format sai

**Fix:**
```yaml
# ‚ùå SAI
slots:
  user_age:
    type: text

# ‚úÖ ƒê√öNG
slots:
  user_age:
    type: text
    mappings:
      - type: from_entity
        entity: age
```

---

### 10. Error: "NLU training takes too long"

**L·ªói:**
```
Training NLU model... (stuck for > 30 min)
```

**Nguy√™n nh√¢n:** Too much training data ho·∫∑c m√°y y·∫øu

**Fix:**
```bash
# Gi·∫£m epochs trong config.yml
DIETClassifier:
  epochs: 50  # Gi·∫£m t·ª´ 100

# Ho·∫∑c cancel & train l·∫°i:
# Ctrl+C
rasa train
```

---

## üü¢ Runtime Issues

### 11. Error: "Bot shell connection refused"

**L·ªói:**
```
Failed to connect to Rasa Core server
```

**Nguy√™n nh√¢n:** Action server ho·∫∑c model kh√¥ng load

**Fix:**
```bash
# Terminal 1: Ollama
ollama serve

# Terminal 2: Action server
rasa run actions --port 5055

# Terminal 3: Bot shell
rasa shell

# N·∫øu v·∫´n l·ªói:
# Check l·∫°i model file c√≥ t·ªìn t·∫°i:
ls models/
```

---

### 12. Error: "Action server not responding"

**L·ªói:**
```
Handler error in webhook URL: Connection to action server failed
```

**Nguy√™n nh√¢n:** Action server ch∆∞a started

**Fix:**
```bash
# Start action server
rasa run actions --port 5055

# Ki·ªÉm tra
curl http://localhost:5055/health

# Output: OK
```

---

### 13. Error: "No response from bot"

**L·ªói:**
```
> hello
(no response or very slow)
```

**Nguy√™n nh√¢n:** Model file b·ªã l·ªói, ho·∫∑c LLM fallback hung

**Fix:**
```bash
# 1. Check model
ls -la models/
# N·∫øu file qu√° nh·ªè (< 100MB), model l·ªói

# 2. Train l·∫°i
rasa train

# 3. Check Ollama
ollama list

# 4. Restart t·∫•t c·∫£ services
# Ctrl+C ·ªü m·ªói terminal
# Restart t·ª´ Terminal 1

# 5. Test response time
# N·∫øu > 5s, c√≥ th·ªÉ LLM query b·ªã hang
```

---

## üî¥ LLM Integration Issues

### 14. Error: "Mistral LLM timeout"

**L·ªói:**
```
requests.exceptions.ConnectTimeout: Connection to Mistral LLM timed out
```

**Nguy√™n nh√¢n:** Ollama ho·∫∑c model qu√° ch·∫≠m

**Fix:**
```python
# actions/utils.py
# Increase timeout
response = requests.post(
    self.api_endpoint,
    json=payload,
    timeout=60  # T·ª´ 30s l√™n 60s
)

# Ho·∫∑c ki·ªÉm tra:
# nvidia-smi  # Check GPU usage
# free -h     # Check RAM
```

---

### 15. Error: "LLM response is empty or gibberish"

**L·ªói:**
```
Bot response: "„ÅÇ„ÅÇ„ÅÇ„ÅÇ" or blank
```

**Nguy√™n nh√¢n:** Mistral model issue, ho·∫∑c prompt sai

**Fix:**
```bash
# Test Mistral tr·ª±c ti·∫øp
ollama run mistral
# G√µ c√¢u ti·∫øng Vi·ªát

# N·∫øu output sai, model b·ªã l·ªói:
ollama pull mistral  # Re-download

# Ho·∫∑c check UNIGROW_SYSTEM_PROMPT trong actions/utils.py
# ƒê·∫£m b·∫£o prompt c√≥ ti·∫øng Vi·ªát
```

---

## üü° Data & Training Issues

### 16. Error: "Training data format invalid"

**L·ªói:**
```
Error in 'data/nlu/intents.yml': Invalid YAML format
```

**Nguy√™n nh√¢n:** YAML syntax error

**Fix:**
```yaml
# ‚ùå SAI (tab indentation)
- intent: greet
	examples: |

# ‚úÖ ƒê√öNG (2 spaces)
- intent: greet
  examples: |
    - hello
    - xin ch√†o
```

---

### 17. Error: "Intent recognition accuracy low"

**L·ªói:**
```
User: "xin ch√†o"
Bot intent: "ask_price" (confidence: 0.45)
```

**Nguy√™n nh√¢n:** Training data kh√¥ng ƒë·ªß

**Fix:**
```yaml
# Th√™m more examples v√†o intents.yml
- intent: greet
  examples: |
    - xin ch√†o
    - hello
    - ch√†o
    - hi
    - ch√†o bu·ªïi s√°ng
    - ch√†o b·∫°n
    - hey
    - alo
    (T·ªëi thi·ªÉu 10-15 examples per intent)

# R·ªìi train l·∫°i
rasa train
```

---

## üìä Performance Issues

### 18. Error: "Response time too slow (> 5s)"

**L·ªói:**
```
User sends message ‚Üí 10+ seconds delay
```

**Nguy√™n nh√¢n:** LLM query ho·∫∑c model too large

**Fix:**
```bash
# 1. Check GPU
nvidia-smi
# N·∫øu GPU usage 100%, b·ªã bottleneck

# 2. Reduce model size
ollama pull tinyllama  # Lightweight model

# 3. Increase GPU VRAM
# N·∫øu m√°y c√≥, upgrade RTX 3060 (12GB)

# 4. Check CPU
top  # Linux/Mac
taskmgr  # Windows
# N·∫øu CPU usage 100%, tune config.yml

# 5. Optimize config.yml
DIETClassifier:
  epochs: 50  # Reduce
TEDPolicy:
  epochs: 50  # Reduce
```

---

### 19. Error: "Out of memory (OOM)"

**L·ªói:**
```
RuntimeError: CUDA out of memory
```

**Nguy√™n nh√¢n:** GPU memory kh√¥ng ƒë·ªß

**Fix:**
```bash
# 1. Check GPU memory
nvidia-smi
# N·∫øu < 4GB free, tutup ·ª©ng d·ª•ng kh√°c

# 2. Close browser, IDE, etc
# Free up RAM

# 3. Restart Ollama
ollama serve

# 4. Use smaller model
ollama pull tinyllama

# 5. Reduce batch size (trong config.yml)
DIETClassifier:
  batch_size: 16  # Reduce
```

---

## üü¢ API Issues

### 20. Error: "API endpoint 404"

**L·ªói:**
```
curl http://localhost:5005/chat
# 404 Not Found
```

**Nguy√™n nh√¢n:** Wrong endpoint

**Fix:**
```bash
# ‚úÖ ƒê√öNG endpoint
curl http://localhost:5005/webhooks/rest/webhook \
  -H "Content-Type: application/json" \
  -d '{"sender": "test", "message": "hello"}'

# Phase 4 s·∫Ω add /chat endpoint
```

---

### 21. Error: "CORS error in frontend"

**L·ªói:**
```
Access to XMLHttpRequest has been blocked by CORS policy
```

**Nguy√™n nh√¢n:** Cross-origin request blocked

**Fix:**
```python
# app.py (Phase 4)
from flask_cors import CORS

app = Flask(__name__)
CORS(app)  # Enable CORS

# Or specific origins:
CORS(app, origins=["http://localhost:3000"])
```

---

## üõ†Ô∏è Debug Mode

### Enable Debug Logging

```python
# actions/actions.py
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

logger.debug(f"Processing: {message}")
logger.info(f"Intent: {intent}")
logger.error(f"Error: {error}")
```

```bash
# Run with debug
rasa shell --debug

# Or
rasa run actions --debug --port 5055
```

---

## üìû Getting Help

### When Troubleshooting Fails

1. **Check Logs:**
   - Rasa logs: Terminal output
   - Ollama logs: `~/.ollama/logs`
   - Python errors: Stack trace in terminal

2. **Collect Debug Info:**
   ```bash
   python --version
   rasa --version
   ollama list
   nvidia-smi
   pip list
   ```

3. **Reproduce Issue:**
   - Note exact steps
   - Expected vs actual output
   - Error message

4. **Report Issue:**
   - Include debug info
   - Attach logs/screenshots
   - Describe environment (Windows/Mac/Linux, GPU, RAM)

---

## ‚úÖ Verification Checklist

After fixing, verify with:

```bash
# 1. Python OK?
python --version

# 2. Venv OK?
pip list | grep rasa

# 3. Rasa OK?
rasa --version

# 4. Ollama OK?
ollama list

# 5. Action server running?
curl http://localhost:5055/health

# 6. Bot responds?
rasa shell
> hello
```

---

**Troubleshooting Guide Ho√†n Th√†nh! üõ†Ô∏è**
